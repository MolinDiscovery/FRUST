{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be349d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frust/pipes/run_ts_per_rpos.py\n",
    "from pathlib import Path\n",
    "from frust.stepper import Stepper\n",
    "from frust.embedder import embed_ts, embed_mols\n",
    "from frust.transformers import transformer_mols\n",
    "from rdkit.Chem.rdchem import Mol\n",
    "import os\n",
    "import inspect\n",
    "import pandas as pd\n",
    "\n",
    "# ─── SHARED SETTINGS (inherit across steps) ─────────────────────────────\n",
    "FUNCTIONAL = \"PBE\" # \"wB97X-D3\"\n",
    "BASISSET = \"def2-SVP\" # \"6-31G**\"\n",
    "BASISSET_SOLV = \"6-31+G**\"  # for solvent SP\n",
    "\n",
    "name = \"run_ts_per_rpos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "52e9e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frust.utils.mols import create_ts_per_rpos\n",
    "\n",
    "job_inputs = create_ts_per_rpos([\"CN1C=CC=C1\"], \"../structures/ts1.xyz\")\n",
    "job_inputs = job_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5e57043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     if \"SLURM_JOB_ID\" in os.environ:\n",
    "#         from nuse import start_monitoring\n",
    "#         start_monitoring(filter_cgroup=True)\n",
    "# except ImportError:\n",
    "#     pass\n",
    "\n",
    "def _best_rows(df):\n",
    "    last_energy = [c for c in df.columns if c.endswith(\"_energy\")][-1]\n",
    "    return (df.sort_values([\"ligand_name\", \"rpos\", last_energy])\n",
    "              .groupby([\"ligand_name\", \"rpos\"]).head(1))\n",
    "\n",
    "\n",
    "def run_init(\n",
    "    ts_struct: dict[str, tuple[Mol, list, str]],\n",
    "    *,\n",
    "    n_confs: int | None = None,\n",
    "    n_cores: int = 4,\n",
    "    mem_gb: int = 20,\n",
    "    debug: bool = False,\n",
    "    top_n: int = 10,\n",
    "    out_dir: str | None = None,\n",
    "    work_dir: str | None = None,\n",
    "    save_output_dir: bool = True,\n",
    "):\n",
    "    import re\n",
    "    pattern = re.compile(\n",
    "    r'^(?:(?P<prefix>(?:TS|INT)\\d*|Mols)\\()?'\n",
    "    r'(?P<ligand>.+?)_rpos\\('        \n",
    "    r'(?P<rpos>\\d+)\\)\\)?$'           \n",
    "    )\n",
    "\n",
    "    name = list(ts_struct.keys())[0]\n",
    "    m = pattern.match(name)\n",
    "    ts_type = m.group(\"prefix\")\n",
    "    \n",
    "    embedded = embed_ts(ts_struct, ts_type=ts_type, n_confs=n_confs, optimize=not debug)\n",
    "\n",
    "    ligand_smiles = list(ts_struct.values())[0][2]\n",
    "\n",
    "    step = Stepper(\n",
    "    ligand_smiles,\n",
    "    n_cores=n_cores,\n",
    "    memory_gb=mem_gb,\n",
    "    debug=debug,\n",
    "    output_base=out_dir,\n",
    "    save_calc_dirs=False,\n",
    "    save_output_dir=save_output_dir,\n",
    "    work_dir=work_dir,\n",
    "    )\n",
    "    \n",
    "    df = step.build_initial_df(embedded)\n",
    "    df = step.xtb(df, options={\"gfnff\": None, \"opt\": None}, constraint=True, save_step=True)\n",
    "    # df = step.xtb(df, options={\"gfn\": 2})\n",
    "    # df = step.xtb(df, options={\"gfn\": 2, \"opt\": None}, constraint=True, lowest=top_n)\n",
    "\n",
    "    # df = step.orca(df, name=\"DFT-pre-SP\", options={\n",
    "    #     FUNCTIONAL  : None,\n",
    "    #     BASISSET    : None,\n",
    "    #     \"TightSCF\"  : None,\n",
    "    #     \"SP\"        : None,\n",
    "    #     \"NoSym\"     : None,\n",
    "    # })\n",
    "\n",
    "    last_energy = [c for c in df.columns if c.endswith(\"_energy\")][-1]\n",
    "    df = (df.sort_values([\"ligand_name\", \"rpos\", last_energy]\n",
    "                        ).groupby([\"ligand_name\", \"rpos\"]).head(1))\n",
    "    \n",
    "    fn_name = inspect.currentframe().f_code.co_name\n",
    "    parquet_name = fn_name.split(\"_\")[1]\n",
    "    df.to_parquet(f\"{out_dir}/{parquet_name}.parquet\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def run_hess(\n",
    "    parquet_path: str,\n",
    "    *,\n",
    "    n_cores: int = 2,\n",
    "    mem_gb: int = 32,\n",
    "    debug: bool = False,\n",
    "    out_dir: str | None = None,\n",
    "    work_dir: str | None = None,\n",
    "):\n",
    "    \n",
    "    df = pd.read_parquet(f\"{name}/{parquet_path}\")\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df = _best_rows(df)\n",
    "\n",
    "    ligand_smiles = list(dict.fromkeys(df[\"smiles\"].tolist()))\n",
    "    step = Stepper(\n",
    "        ligand_smiles,\n",
    "        n_cores=n_cores,\n",
    "        memory_gb=mem_gb,\n",
    "        debug=debug,\n",
    "        output_base=out_dir,\n",
    "        save_calc_dirs=False,\n",
    "        save_output_dir=True,\n",
    "        work_dir=work_dir,\n",
    "    )\n",
    "\n",
    "    df = step.orca(df, name=\"Hess\", options={\n",
    "        \"XTB2\": None,\n",
    "        #BASISSET: None,\n",
    "        #\"TightSCF\": None,\n",
    "        \"Freq\": None,\n",
    "        \"NoSym\": None,\n",
    "    }, read_files=[\"input.hess\"])\n",
    "\n",
    "    stem = parquet_path.rsplit('.', 1)[0]\n",
    "    out_parquet = stem + \".hess.parquet\"\n",
    "    df.to_parquet(f\"{name}/{out_parquet}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_OptTS(\n",
    "    parquet_path: str,\n",
    "    *,\n",
    "    n_cores: int = 8,\n",
    "    mem_gb: int = 40,\n",
    "    debug: bool = False,\n",
    "    out_dir: str | None = None,\n",
    "    work_dir: str | None = None,\n",
    "):\n",
    "    df = pd.read_parquet(f\"{name}/{parquet_path}\")\n",
    "\n",
    "    ligand_smiles = list(dict.fromkeys(df[\"smiles\"].tolist()))\n",
    "    step = Stepper(\n",
    "        ligand_smiles,\n",
    "        n_cores=n_cores,\n",
    "        memory_gb=mem_gb,\n",
    "        debug=debug,\n",
    "        output_base=out_dir,\n",
    "        save_calc_dirs=True,\n",
    "        save_output_dir=True,\n",
    "        work_dir=work_dir,\n",
    "    )\n",
    "\n",
    "    # Read previously computed Hessian (*.hess from ORCA)\n",
    "    df = step.orca(df, name=\"DFT-OptTS\", options={\n",
    "        \"XTB2\": None,\n",
    "        #BASISSET: None,\n",
    "        \"TightSCF\": None,\n",
    "        #\"SlowConv\": None,\n",
    "        \"OptTS\": None,\n",
    "        #\"NoSym\": None,\n",
    "    }, use_last_hess=True)\n",
    "\n",
    "    stem = parquet_path.rsplit('.', 1)[0]\n",
    "    out_parquet = stem + \".optts.parquet\"\n",
    "    df.to_parquet(f\"{name}/{out_parquet}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_freq(\n",
    "    parquet_path: str,\n",
    "    *,\n",
    "    n_cores: int = 8,\n",
    "    mem_gb: int = 40,\n",
    "    debug: bool = False,\n",
    "    out_dir: str | None = None,\n",
    "    work_dir: str | None = None,        \n",
    "):\n",
    "    df = pd.read_parquet(f\"{name}/{parquet_path}\")\n",
    "\n",
    "    ligand_smiles = list(dict.fromkeys(df[\"smiles\"].tolist()))\n",
    "    step = Stepper(\n",
    "        ligand_smiles,\n",
    "        n_cores=n_cores,\n",
    "        memory_gb=mem_gb,\n",
    "        debug=debug,\n",
    "        output_base=out_dir,\n",
    "        save_calc_dirs=True,\n",
    "        save_output_dir=True,\n",
    "        work_dir=work_dir,\n",
    "    )\n",
    "\n",
    "    df = step.orca(df, name=\"DFT-OptTS\", options={\n",
    "        \"XTB2\": None,\n",
    "        #BASISSET: None,\n",
    "        \"TightSCF\": None,\n",
    "        #\"SlowConv\": None,\n",
    "        \"Freq\": None,\n",
    "        #\"NoSym\": None,\n",
    "    })\n",
    "\n",
    "    stem = parquet_path.rsplit('.', 1)[0]\n",
    "    out_parquet = stem + \".freq.parquet\"\n",
    "    df.to_parquet(f\"{name}/{out_parquet}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_solv(\n",
    "    parquet_path: str,\n",
    "    *,\n",
    "    n_cores: int = 8,\n",
    "    mem_gb: int = 40,\n",
    "    debug: bool = False,\n",
    "    out_dir: str | None = None,\n",
    "    work_dir: str | None = None,        \n",
    "):\n",
    "    df = pd.read_parquet(f\"{name}/{parquet_path}\")\n",
    "\n",
    "    ligand_smiles = list(dict.fromkeys(df[\"smiles\"].tolist()))\n",
    "    step = Stepper(\n",
    "        ligand_smiles,\n",
    "        n_cores=n_cores,\n",
    "        memory_gb=mem_gb,\n",
    "        debug=debug,\n",
    "        output_base=out_dir,\n",
    "        save_calc_dirs=True,\n",
    "        save_output_dir=True,\n",
    "        work_dir=work_dir,\n",
    "    )\n",
    "\n",
    "    df = step.orca(df, name=\"DFT-solv\", options={\n",
    "        \"XTB2\": None,\n",
    "        #BASISSET: None,\n",
    "        \"TightSCF\": None,\n",
    "        #\"SlowConv\": None,\n",
    "        \"SP\": None,\n",
    "        #\"NoSym\": None,\n",
    "    })\n",
    "\n",
    "    stem = parquet_path.rsplit('.', 1)[0]\n",
    "    out_parquet = stem + \".solv.parquet\"\n",
    "    df.to_parquet(f\"{name}/{out_parquet}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def run_cleanup(out_dir):\n",
    "    print(\"[INFO]: Cleanup initiated...\")\n",
    "\n",
    "    \"\"\"Deletes residual .parquet files\"\"\"\n",
    "    parquet_files = list(Path(out_dir).glob(\"*.parquet\"))\n",
    "    if not parquet_files:\n",
    "        print(\"No parquet files found.\")\n",
    "        return\n",
    "\n",
    "    if len(parquet_files) < 2:\n",
    "        print(\"Passing parquet cleanup, only 1 parquat file found.\")\n",
    "        return\n",
    "    \n",
    "    parquet_len = {}\n",
    "    for f in parquet_files:\n",
    "        c = len(f.name.split('.'))\n",
    "        parquet_len[f] = c\n",
    "        \n",
    "    parquet_len_sorted = dict(sorted(parquet_len.items(), key=lambda x: x[1], reverse=False))\n",
    "    dont_delete = max(parquet_len_sorted.values())\n",
    "\n",
    "    for f, c in parquet_len_sorted.items():\n",
    "        if c < dont_delete:\n",
    "            print(f\"[INFO]: Removing residual parquet file {f}\")\n",
    "            Path(f).unlink()\n",
    "\n",
    "    print(\"[INFO]: Cleanup done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "61f07710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 1 conformers on atom 44\n",
      "2025-11-04 15:19:24 INFO  frust.stepper: Working dir: .\n",
      "2025-11-04 15:19:24 INFO  frust.stepper: [xtb-gfnff-opt] row 0 (TS1(1-methylpyrrole_rpos(2)))…\n",
      "2025-11-04 15:19:25 INFO  frust.stepper: Working dir: .\n",
      "2025-11-04 15:19:25 INFO  frust.stepper: [Hess-XTB2-Freq-NoSym] row 0 (TS1(1-methylpyrrole_rpos(2)))…\n",
      "2025-11-04 15:19:29 INFO  frust.stepper: Working dir: .\n",
      "2025-11-04 15:19:29 INFO  frust.stepper: [DFT-OptTS-XTB2-TightSCF-OptTS] row 0 (TS1(1-methylpyrrole_rpos(2)))…\n",
      "2025-11-04 15:19:35 INFO  frust.stepper: Working dir: .\n",
      "2025-11-04 15:19:35 INFO  frust.stepper: [DFT-OptTS-XTB2-TightSCF-Freq] row 0 (TS1(1-methylpyrrole_rpos(2)))…\n",
      "2025-11-04 15:19:40 INFO  frust.stepper: Working dir: .\n",
      "2025-11-04 15:19:40 INFO  frust.stepper: [DFT-solv-XTB2-TightSCF] row 0 (TS1(1-methylpyrrole_rpos(2)))…\n"
     ]
    }
   ],
   "source": [
    "_ = run_init(job_inputs, n_confs=1, out_dir=name)\n",
    "_ = run_hess(\"init.parquet\", out_dir=name, n_cores=10)\n",
    "_ = run_OptTS(\"init.hess.parquet\", out_dir=name, n_cores=10)\n",
    "_ = run_freq(\"init.hess.optts.parquet\", out_dir=name, n_cores=10)\n",
    "_ = run_solv(\"init.hess.optts.freq.parquet\", out_dir=name, n_cores=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "61b9dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Cleanup initiated...\n",
      "[INFO]: Removing residual parquet file run_ts_per_rpos/init.parquet\n",
      "[INFO]: Removing residual parquet file run_ts_per_rpos/init.hess.parquet\n",
      "[INFO]: Removing residual parquet file run_ts_per_rpos/init.hess.optts.parquet\n",
      "[INFO]: Removing residual parquet file run_ts_per_rpos/init.hess.optts.freq.parquet\n",
      "[INFO]: Cleanup done!\n"
     ]
    }
   ],
   "source": [
    "run_cleanup(out_dir=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
